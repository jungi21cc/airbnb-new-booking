{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_setting():\n",
    "    last_train_data_add = pd.read_csv(\"last_train.csv\")\n",
    "    last_test_data_add = pd.read_csv(\"last_test.csv\")\n",
    "    \n",
    "    X = last_train_data_add\n",
    "    y = last_test_data_add\n",
    "    \n",
    "#     clf1 = linear_model.LogisticRegression(n_jobs=-1)\n",
    "#     clf2 = RandomForestClassifier(n_jobs=-1)\n",
    "#     clf3 = ExtraTreesClassifier(n_jobs=-1)\n",
    "#     clf4 = xgb.XGBClassifier(nthread=3, n_jobs=-1)\n",
    "    \n",
    "#     eclf2 = VotingClassifier(estimators=[('log', clf1), ('rf', clf2), ('ex', clf3), ('xgb', clf4), ('lgb', clf5)], voting='soft', weights=[1,1,1,1,1])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = pre_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_kaggle(df_train, df_test, user_id, target):\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "\n",
    "    y_train = le.fit_transform(df_train[target])\n",
    "    X_train = df_train.drop([target, user_id], axis = 1)\n",
    "    \n",
    "    X_test_id = df_test[user_id]\n",
    "    X_test = df_test.drop([target, user_id], axis = 1)\n",
    "    \n",
    "    print(\"sklearn cross validation 시작\")\n",
    "    model1 = lgb.LGBMClassifier(nthread=3, n_jobs=-1)\n",
    "    scores1 = cross_val_score(estimator=model1, X=X_train, y=y_train, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(\"log_loss: %0.2f (+/- %0.2f)\" % (scores1.mean(), scores1.std() * 2), 'default')\n",
    "    print()\n",
    "    \n",
    "    print(\"sklearn cross validation 시작\")\n",
    "    model2 = lgb.LGBMClassifier(nthread=3, n_jobs=-1, reg_alpha=1)\n",
    "    scores2 = cross_val_score(estimator=model2, X=X_train, y=y_train, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(\"log_loss: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2), 'reg_alpha=1')\n",
    "    print()\n",
    "    \n",
    "    print(\"sklearn cross validation 시작\")\n",
    "    model3 = lgb.LGBMClassifier(nthread=3, n_jobs=-1, reg_alpha=0.5)\n",
    "    scores3 = cross_val_score(estimator=model3, X=X_train, y=y_train, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(\"log_loss: %0.2f (+/- %0.2f)\" % (scores3.mean(), scores3.std() * 2), 'reg_alpha=0.5')\n",
    "    print()\n",
    "    \n",
    "    print(\"sklearn cross validation 시작\")\n",
    "    model4 = lgb.LGBMClassifier(nthread=3, n_jobs=-1, reg_lambda=0.5)\n",
    "    scores4 = cross_val_score(estimator=model4, X=X_train, y=y_train, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(\"log_loss: %0.2f (+/- %0.2f)\" % (scores4.mean(), scores4.std() * 2), 'reg_lambda=0.5')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(\"sklearn cross validation 시작\")\n",
    "    model5 = lgb.LGBMClassifier(nthread=3, n_jobs=-1, reg_alpha=0.5, reg_lambda=0.5)\n",
    "    scores5 = cross_val_score(estimator=model5, X=X_train, y=y_train, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(\"log_loss: %0.2f (+/- %0.2f)\" % (scores5.mean(), scores5.std() * 2), 'reg_lambda=1')\n",
    "           \n",
    "    \n",
    "    print(\"sklearn cross validation 시작\")\n",
    "    model6 = lgb.LGBMClassifier(nthread=3, n_jobs=-1, reg_lambda=1)\n",
    "    scores6 = cross_val_score(estimator=model5, X=X_train, y=y_train, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(\"log_loss: %0.2f (+/- %0.2f)\" % (scores6.mean(), scores6.std() * 2), 'reg_lambda=1')\n",
    "    \n",
    "        \n",
    "    print(\"sklearn cross validation 시작\")\n",
    "    model7 = lgb.LGBMClassifier(nthread=3, n_jobs=-1, reg_alpha=1, learning_rate=0.05, n_estimators=500)\n",
    "    scores7 = cross_val_score(estimator=model5, X=X_train, y=y_train, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(\"log_loss: %0.2f (+/- %0.2f)\" % (scores7.mean(), scores7.std() * 2), 'reg_lambda=1, learning_rate=0.05, n_estimator=500')\n",
    "    \n",
    "            \n",
    "    print(\"sklearn cross validation 시작\")\n",
    "    model8 = lgb.LGBMClassifier(nthread=3, n_jobs=-1, reg_alpha=2, learning_rate=0.05, n_estimators=500)\n",
    "    scores8 = cross_val_score(estimator=model5, X=X_train, y=y_train, cv=4, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(\"log_loss: %0.2f (+/- %0.2f)\" % (scores8.mean(), scores8.std() * 2), 'reg_lambda=2, learning_rate=0.05, n_estimator=500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submit_kaggle(X, y, \"id\", \"country_destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
